<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         Having issue with understanding Backpropagation?
        
    </title><meta content="Having issue with understanding Backpropagation?" property=og:title><meta content="This is an example description" property=og:description><meta content="This is an example description" name=description><link href=/icon/favicon.png rel=icon type=image/png><link href=https://aminehd.github.io/tech-content-site/fonts.css rel=stylesheet><script src=https://aminehd.github.io/tech-content-site/js/codeblock.js></script><script src=https://aminehd.github.io/tech-content-site/js/toc.js></script><script src=https://aminehd.github.io/tech-content-site/js/note.js></script><script>MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
              }
            };</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link href=" https://aminehd.github.io/tech-content-site/atom.xml" title="Amineh Dadsetan" rel=alternate type=application/atom+xml><link href=https://aminehd.github.io/tech-content-site/theme/light.css rel=stylesheet><link href=" https://aminehd.github.io/tech-content-site/theme/dark.css" id=darkModeStyle rel=stylesheet><script src=https://aminehd.github.io/tech-content-site/js/themetoggle.js></script><script>setTheme(getSavedTheme());</script><link href=https://aminehd.github.io/tech-content-site/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><a href=https://aminehd.github.io/tech-content-site/>Amineh Dadsetan</a><div class=socials><a class=social href=https://github.com/aminehd/ rel=me> <img alt=github src=https://aminehd.github.io/tech-content-site/social_icons/github.svg> </a></div></div><nav><a href=https://aminehd.github.io/tech-content-site/posts style=margin-left:.5em>/posts</a> |<a onclick="toggleTheme(); event.preventDefault();" href=# id=dark-mode-toggle> <img alt=Light id=sun-icon src=https://aminehd.github.io/tech-content-site/feather/sun.svg style=filter:invert()> <img alt=Dark id=moon-icon src=https://aminehd.github.io/tech-content-site/feather/moon.svg> </a><script>updateItemToggleTheme()</script></nav></header><main><article><div class=title><div class=page-header>Having issue with understanding Backpropagation?<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2024-12-30</time><span class=tags-label> :: Tags:</span><span class=tags> <a href=" https://aminehd.github.io/tech-content-site/tags/documentation/" class=post-tag>documentation</a> </span> :: <a rel="noopener noreferrer" href=https://github.com/aminehd/tech-content-site/tree/main/contentposts/BackProp.md target=_blank> Source Code</a></div></div><div class=toc-container><h1 class=toc-title>Table of Contents</h1><ul class=toc-list><li><a href=" https://aminehd.github.io/tech-content-site/posts/backprop/#backpropagation-in-neural-networks">Backpropagation in Neural Networks</a></ul></div><section class=body><h1 id=backpropagation-in-neural-networks><a aria-label="Anchor link for: backpropagation-in-neural-networks" class=zola-anchor href=#backpropagation-in-neural-networks>Backpropagation in Neural Networks</a></h1><p>Backpropagation is a gamified version of the chain rule. The general framework is to see a complex function (neural net) as a combination of simple functions, a.k.a gates. Then calculate how change in parameters, flow through these gates all the way to the <em>loss function</em>.</p><img alt="Complex function as combination of gates" id=BackProp_gates src=https://raw.githubusercontent.com/aminehd/tech-content-site/main/content/images/BackProp_gates.png><p>Backpropagation is not a single too intellectually challenging concept. It is a combination of too many concepts and frameworks. Thus not knowing any of them fully can make it hard to understand.<p>If you have ever found yourself struggling to understand backpropagation, follow my questions below. Try to answer them. If you find them vague, read the resources I have linked. Don't get stuck on any one question. Rather try to read through and come back to the question later.<ul><li><p>üåü <strong>Do you know the difference between the Total Derivative $ df/dt $ and the Partial Derivative $ \partial f/\partial t $?</strong><br> $ df/dt $ accounts for all the ways in which $ f $ changes as $ t $ changes, while $ \partial f/\partial t $ only considers the direct effect of $ t $ on $ f $. For functions of a single variable, $ df/dt $ and $ \partial f/\partial t $ are essentially the same. See below example and read more on <a href=https://math.stackexchange.com/questions/2277214/the-difference-between-frac-dfdt-and-frac-partial-f-partial-t>Math StackExchange</a>.</p> <p>For example for a function $ f(x(t), y(t), t) $: $$ \frac{df}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} + \frac{\partial f}{\partial t} $$</p><li><p>üîó <strong>Have you heard of the Chain Rule?</strong> Does chain rule apply for partial derivatives or total derivatives?</p><li><p>üìê <strong>Do you know how to move from simple derivatives to vectorized ones?</strong><br> Read about <a href=https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf>Vectorized Derivatives</a>. Pay close attention to the "Useful Identities" section.<br> For a matrix $ A $, vector $ x $, and scalar $ a $, we have the following identity, where $x = (x_1, ..., x_n) \in \mathbb{R}^n$ and $f(x) \in \mathbb{R}_m$:<br> $$ \frac{\partial f}{\partial x} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \ \vdots & \ddots & \vdots \ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix} $$</p> <blockquote><p>Is $ \frac{\partial f}{\partial x} $ equal to $ \frac{df}{dx} $?</blockquote> <blockquote><p>What is the dimension of $ \frac{\partial f}{\partial x} $?</blockquote> <p>$\frac{\partial f}{\partial x}$ is called the Jacobian matrix. Do you know Jacobian's matrix is a linear transformation in the space of functions? Wait what <em>Function Space</em>? ü§Ø</p><li><p>üìö <strong>Have you read Andrej Karpathy's <a href=https://cs231n.github.io/optimization-2/#intro>Intuitive Understanding of Backpropagation</a>?</strong><br> Have you tried to understand what he means by local gradients and backpropagation being a local process? What about staged backpropagation?</p><li><p>üß† <strong>Do you know cool tricks to calculate complex derivatives?</strong><br> For example, how do you calculate the derivative of the sigmoid function? Here it is (though I didn‚Äôt prove it‚Äîyou can!):<br> $$ \sigma(x) = \frac{1}{1 + e^{-x}}\ \frac{d\sigma}{dx} = \sigma(x) \cdot (1 - \sigma(x)) $$</p><li><p>üß† <strong>Do you want to see 5 lines of code that summerizes it?</strong> No problem</p></ul><pre class=language-python data-lang=python style=color:#61676c;background-color:#fafafa><code class=language-python data-lang=python><span style=color:#abb0b6;font-style:italic># Forward pass
</span><span>x </span><span style=color:#ed9366>= </span><span>gate1</span><span style=color:#ed9366>.</span><span style=color:#f29718>forward</span><span>(a)
</span><span>y </span><span style=color:#ed9366>= </span><span>gate2</span><span style=color:#ed9366>.</span><span style=color:#f29718>forward</span><span>(x)
</span><span>
</span><span style=color:#abb0b6;font-style:italic># Backward pass
</span><span>dL_dy </span><span style=color:#ed9366>= </span><span style=color:#ff8f40>... </span><span style=color:#abb0b6;font-style:italic># gradient of loss w.r.t. final output
</span><span>dL_dx </span><span style=color:#ed9366>= </span><span>gate2</span><span style=color:#ed9366>.</span><span style=color:#f29718>backward</span><span>(dL_dy) </span><span style=color:#abb0b6;font-style:italic># ‚àÇL/‚àÇx = (‚àÇL/‚àÇy) * (‚àÇy/‚àÇx)
</span><span>dL_da </span><span style=color:#ed9366>= </span><span>gate1</span><span style=color:#ed9366>.</span><span style=color:#f29718>backward</span><span>(dL_dx) </span><span style=color:#abb0b6;font-style:italic># ‚àÇL/‚àÇa = (‚àÇL/‚àÇx) * (‚àÇx/‚àÇa)
</span><span>
</span></code></pre></section></article></main><div class=giscus></div><script async crossorigin issue-term=pathname repo=not-matthias/apollo src=https://utteranc.es/client.js theme=github-light></script></div>